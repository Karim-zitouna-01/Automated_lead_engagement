import os
import nltk
from dotenv import load_dotenv
from pathlib import Path

# T√©l√©charger les ressources n√©cessaires de NLTK (une seule fois)
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import chromadb
from langchain_google_genai import ChatGoogleGenerativeAI


# Classe de gestion de Gemini
class LLMManager:
    def __init__(self):
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            api_key=os.getenv("GEMINI_API_KEY")
        )

    def ask_with_context(self, question, context):
        prompt = f"""R√©ponds pr√©cis√©ment √† la question ci-dessous en te basant uniquement sur ce contexte :

=== CONTEXTE ===
{context}

=== QUESTION ===
{question}
"""
        return self.llm.invoke(prompt).content

def main():
    load_dotenv()

    # --- Dossiers ---
    data_root = Path("outputs")  # Contient les rapports .md
    db_path = Path(".chroma_db")

    if not data_root.exists():
        print("‚ùå Aucun rapport trouv√© dans le dossier 'outputs'.")
        return

    # --- Chargement des rapports markdown (.md) ---
    loader = DirectoryLoader(str(data_root), glob="**/*.md")
    raw_documents = loader.load()

    if not raw_documents:
        print("‚ùå Aucun fichier Markdown √† indexer.")
        return

    # --- Chunking ---
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=300,
    )
    chunks = text_splitter.split_documents(raw_documents)

    # --- Vectorisation via ChromaDB ---
    chroma_client = chromadb.PersistentClient(path=str(db_path))
    collection = chroma_client.get_or_create_collection(
        name="lead_reports_collection",
        metadata={"hnsw:space": "cosine"}
    )

    documents = [chunk.page_content for chunk in chunks]
    metadatas = [chunk.metadata for chunk in chunks]
    ids = [f"report_{i}" for i in range(len(chunks))]
    collection.upsert(documents=documents, metadatas=metadatas, ids=ids)
    print(f"‚úÖ {len(documents)} rapports index√©s dans ChromaDB.")

    # --- Initialisation LLM ---
    llm_manager = LLMManager()

    # --- Boucle Q/R ---
    while True:
        question = input("\nüí¨ Pose une question sur un rapport (ou 'exit') : ")
        if question.lower() in ["exit", "quit"]:
            break

        results = collection.query(query_texts=[question], n_results=5)
        context_chunks = results["documents"][0]
        context = "\n\n".join(context_chunks)

        # --- R√©ponse ---
        answer = llm_manager.ask_with_context(question, context)
        print("\nü§ñ R√©ponse :\n", answer)

if __name__ == "__main__":
    main()
